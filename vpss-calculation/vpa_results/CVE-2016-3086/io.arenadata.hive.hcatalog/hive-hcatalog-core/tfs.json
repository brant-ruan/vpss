{
  "2.3.9_arenadata2": [
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.3.9_arenadata1": [
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.3.9_arenadata3": [
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>"
  ]
}