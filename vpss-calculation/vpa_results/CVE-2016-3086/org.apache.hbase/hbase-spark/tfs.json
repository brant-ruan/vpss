{
  "2.0.0-alpha-1": [
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.hadoop.hbase.spark.HBaseContext$WriterLength org$apache$hadoop$hbase$spark$HBaseContext$$getNewHFileWriter(byte[],org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.HashMap,org.apache.hadoop.hbase.io.compress.Compression$Algorithm)>",
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.spark.rdd.RDD hbaseRDD(org.apache.hadoop.hbase.TableName,org.apache.hadoop.hbase.client.Scan,scala.Function1,scala.reflect.ClassTag)>"
  ],
  "2.0.0-alpha2": [
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.hadoop.hbase.spark.HBaseContext$WriterLength org$apache$hadoop$hbase$spark$HBaseContext$$getNewHFileWriter(byte[],org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.HashMap,org.apache.hadoop.hbase.io.compress.Compression$Algorithm)>",
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.spark.rdd.RDD hbaseRDD(org.apache.hadoop.hbase.TableName,org.apache.hadoop.hbase.client.Scan,scala.Function1,scala.reflect.ClassTag)>"
  ],
  "2.0.0-alpha3": [
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.hadoop.hbase.spark.HBaseContext$WriterLength org$apache$hadoop$hbase$spark$HBaseContext$$getNewHFileWriter(byte[],org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.HashMap,org.apache.hadoop.hbase.io.compress.Compression$Algorithm)>"
  ],
  "2.0.0-alpha4": [
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.hadoop.conf.Configuration getConf(org.apache.spark.broadcast.Broadcast)>",
    "<org.apache.hadoop.hbase.spark.HBaseContext: org.apache.spark.rdd.RDD hbaseRDD(org.apache.hadoop.hbase.TableName,org.apache.hadoop.hbase.client.Scan,scala.Function1,scala.reflect.ClassTag)>",
    "<org.apache.hadoop.hbase.spark.HBaseContext$$anonfun$bulkLoad$3: void apply(scala.collection.Iterator,org.apache.hadoop.hbase.client.Connection)>",
    "<org.apache.hadoop.hbase.spark.HBaseContext: void applyCreds()>",
    "<org.apache.hadoop.hbase.spark.HBaseContext$$anonfun$bulkLoadThinRows$3: void apply(scala.collection.Iterator,org.apache.hadoop.hbase.client.Connection)>",
    "<org.apache.hadoop.hbase.spark.HBaseContext: void <init>(org.apache.spark.SparkContext,org.apache.hadoop.conf.Configuration,java.lang.String)>"
  ]
}