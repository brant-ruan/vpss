{
  "2.3.3": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.2": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.5": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.8": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.9": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.4": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.10": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.1": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.7": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.6": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.3.0": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.0.1": [
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: void setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.0.0": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "2.1.0": [
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.1.1": [
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "1.2.0": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ],
  "1.2.1": [
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>"
  ]
}