{
  "1.2.1.spark2": [
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.security.authorization.Privilege[],org.apache.hadoop.hive.ql.security.authorization.Privilege[])>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorizeDDL(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void postAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: void postAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.metadata.Partition,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>"
  ]
}