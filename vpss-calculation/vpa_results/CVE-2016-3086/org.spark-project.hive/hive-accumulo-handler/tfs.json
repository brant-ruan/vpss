{
  "1.2.1.spark": [
    "<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void mergeTokenIntoJobConf(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters: void <init>(org.apache.hadoop.conf.Configuration,java.util.Properties,java.lang.String)>",
    "<org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator: java.lang.Object processExpression(org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc,java.lang.Object[])>"
  ],
  "1.2.1.spark2": [
    "<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void mergeTokenIntoJobConf(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters: void <init>(org.apache.hadoop.conf.Configuration,java.util.Properties,java.lang.String)>",
    "<org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator: java.lang.Object processExpression(org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc,java.lang.Object[])>"
  ]
}