{
  "1.2.1.spark": [
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCompactions(org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$7: void run()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTxns(org.apache.hadoop.hive.ql.plan.ShowTxnsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int descDatabase(org.apache.hadoop.hive.ql.plan.DescDatabaseDesc)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DescTableDesc)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void replaceFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniSparkCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.DriverContext,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropSessionPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showFunctions(org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.ql.session.SessionState start(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCreateTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void applyConstraints(java.net.URI,java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.ql.io.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showPartitions(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeFunction(org.apache.hadoop.hive.ql.plan.DescFunctionDesc)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,int,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocksNewFormat(org.apache.hadoop.hive.ql.plan.ShowLocksDesc,org.apache.hadoop.hive.ql.lockmgr.HiveLockManager)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: java.lang.String downloadResource(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$ReaderOptions)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.ArrayList)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printJsonData(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTables(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.exec.Task)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showIndexes(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowIndexesDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showColumns(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowColumnsDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showDatabases(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$6: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocks(org.apache.hadoop.hive.ql.plan.ShowLocksDesc)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService)>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection()>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: java.lang.String getDelegationToken(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniTezCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTableStatus(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc)>"
  ],
  "1.2.1.spark2": [
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCompactions(org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$7: void run()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTxns(org.apache.hadoop.hive.ql.plan.ShowTxnsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int descDatabase(org.apache.hadoop.hive.ql.plan.DescDatabaseDesc)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DescTableDesc)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void replaceFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniSparkCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.DriverContext,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropSessionPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showFunctions(org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.ql.session.SessionState start(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCreateTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void applyConstraints(java.net.URI,java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.ql.io.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showPartitions(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeFunction(org.apache.hadoop.hive.ql.plan.DescFunctionDesc)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,int,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocksNewFormat(org.apache.hadoop.hive.ql.plan.ShowLocksDesc,org.apache.hadoop.hive.ql.lockmgr.HiveLockManager)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: java.lang.String downloadResource(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$ReaderOptions)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.ArrayList)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printJsonData(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTables(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.exec.Task)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showIndexes(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowIndexesDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showColumns(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowColumnsDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showDatabases(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$6: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocks(org.apache.hadoop.hive.ql.plan.ShowLocksDesc)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService)>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection()>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: java.lang.String getDelegationToken(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniTezCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTableStatus(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc)>"
  ]
}