{
  "1.2.1.spark": [
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: boolean isHBaseGenerateHFiles(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: java.lang.Class getInputFormatClass()>",
    "<org.apache.hadoop.hive.hbase.HBaseSerDeParameters: void <init>(org.apache.hadoop.conf.Configuration,java.util.Properties,java.lang.String)>",
    "<org.apache.hadoop.hive.hbase.HiveHFileOutputFormat: void checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat: void checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat: org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.hbase.HiveHFileOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: void configureTableJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplitsInternal(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: void configureJobConf(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil: org.apache.hadoop.hbase.client.Scan getScan(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>"
  ],
  "1.2.1.spark2": [
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: boolean isHBaseGenerateHFiles(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: java.lang.Class getInputFormatClass()>",
    "<org.apache.hadoop.hive.hbase.HBaseSerDeParameters: void <init>(org.apache.hadoop.conf.Configuration,java.util.Properties,java.lang.String)>",
    "<org.apache.hadoop.hive.hbase.HiveHFileOutputFormat: void checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat: void checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat: org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.hbase.HiveHFileOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: void configureTableJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplitsInternal(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.hbase.HBaseStorageHandler: void configureJobConf(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil: org.apache.hadoop.hbase.client.Scan getScan(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>"
  ]
}