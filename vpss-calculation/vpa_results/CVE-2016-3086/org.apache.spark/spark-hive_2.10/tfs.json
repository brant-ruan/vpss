{
  "1.6.2": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.1": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.0": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.5.0": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$21: scala.Tuple3 matchSerDe$1(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$3$$anonfun$apply$2: scala.Option apply()>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2: org.apache.spark.sql.catalyst.expressions.Expression apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan nodeToPlan(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry: org.apache.hadoop.hive.ql.exec.FunctionInfo getFunctionInfo(java.lang.String)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.HiveContext$SQLSession: org.apache.hadoop.hive.ql.session.SessionState sessionState$lzycompute()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$: void initializeLocalJobConfFunc(java.lang.String,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: java.lang.Object retryLocked(scala.Function0)>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSerDe$: scala.Option sourceToSerDe(java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "1.5.2": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$3$$anonfun$apply$2: scala.Option apply()>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2: org.apache.spark.sql.catalyst.expressions.Expression apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan nodeToPlan(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$21: scala.Tuple4 matchSerDe$1(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry: org.apache.hadoop.hive.ql.exec.FunctionInfo getFunctionInfo(java.lang.String)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$21: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveContext$SQLSession: org.apache.hadoop.hive.ql.session.SessionState sessionState$lzycompute()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$: void initializeLocalJobConfFunc(java.lang.String,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: java.lang.Object retryLocked(scala.Function0)>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSerDe$: scala.Option sourceToSerDe(java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "1.6.3": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.5.1": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$3$$anonfun$apply$2: scala.Option apply()>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2: org.apache.spark.sql.catalyst.expressions.Expression apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan nodeToPlan(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$21: scala.Tuple4 matchSerDe$1(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry: org.apache.hadoop.hive.ql.exec.FunctionInfo getFunctionInfo(java.lang.String)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$21: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveContext$SQLSession: org.apache.hadoop.hive.ql.session.SessionState sessionState$lzycompute()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$: void initializeLocalJobConfFunc(java.lang.String,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: java.lang.Object retryLocked(scala.Function0)>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSerDe$: scala.Option sourceToSerDe(java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.2": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.1": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.2.0": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Partition toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.execution.HiveOutputWriter: void <init>(java.lang.String,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc,org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.types.StructType)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1: java.lang.String getFileExtension(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Table toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: scala.collection.Seq run(org.apache.spark.sql.SparkSession)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.1.1": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.0": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.1.0": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.2.2": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Partition toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.execution.HiveOutputWriter: void <init>(java.lang.String,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc,org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.types.StructType)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1: java.lang.String getFileExtension(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Table toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: scala.collection.Seq run(org.apache.spark.sql.SparkSession)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.2.3": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Partition toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.execution.HiveOutputWriter: void <init>(java.lang.String,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc,org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.types.StructType)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1: java.lang.String getFileExtension(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Table toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: scala.collection.Seq run(org.apache.spark.sql.SparkSession)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.2.1": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Partition toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.execution.HiveOutputWriter: void <init>(java.lang.String,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc,org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.types.StructType)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1: java.lang.String getFileExtension(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Table toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: scala.collection.Seq run(org.apache.spark.sql.SparkSession)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.conf.Configuration,boolean)>"
  ],
  "2.1.2": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.1.3": [
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.0-preview": [
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.orc.DefaultSource$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ]
}