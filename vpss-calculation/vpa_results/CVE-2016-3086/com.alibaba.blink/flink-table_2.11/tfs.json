{
  "blink-3.2.2": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.6.8": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.2.3": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.7.0": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.2.1": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "1.5.1": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.3.0": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.2.0": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.2.4": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ],
  "blink-3.4.0": [
    "<org.apache.flink.table.sources.orc.OrcInputFormat: org.apache.hadoop.mapreduce.RecordReader createReader(org.apache.flink.core.fs.FileInputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcInputFormat: void open(org.apache.flink.core.fs.FileInputSplit)>",
    "<org.apache.flink.table.sources.orc.OrcVectorizedReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.flink.table.sources.orc.OrcTableStatsCollector$: scala.Tuple2 org$apache$flink$table$sources$orc$OrcTableStatsCollector$$getStatisticsOfFile(org.apache.flink.core.fs.FileStatus,java.lang.String[],org.apache.hadoop.conf.Configuration)>"
  ]
}