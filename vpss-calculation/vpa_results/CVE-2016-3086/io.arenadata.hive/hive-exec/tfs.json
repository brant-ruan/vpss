{
  "2.3.9_arenadata2": [
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void <init>(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: boolean[] genIncludedColumns(org.apache.orc.TypeDescription,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: org.apache.parquet.hadoop.api.ReadSupport$ReadContext init(org.apache.parquet.hadoop.api.InitContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader: org.apache.avro.Schema getSchema(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.io.RCFile$Reader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void <init>(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapred.JobConf)>"
  ],
  "2.3.9_arenadata1": [
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void <init>(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: boolean[] genIncludedColumns(org.apache.orc.TypeDescription,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: org.apache.parquet.hadoop.api.ReadSupport$ReadContext init(org.apache.parquet.hadoop.api.InitContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader: org.apache.avro.Schema getSchema(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.io.RCFile$Reader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void <init>(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapred.JobConf)>"
  ],
  "2.3.9_arenadata3": [
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void <init>(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: boolean[] genIncludedColumns(org.apache.orc.TypeDescription,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: org.apache.parquet.hadoop.api.ReadSupport$ReadContext init(org.apache.parquet.hadoop.api.InitContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader: org.apache.avro.Schema getSchema(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.io.RCFile$Reader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader: void <init>(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapred.JobConf)>"
  ]
}