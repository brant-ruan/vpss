{
  "1.6.1": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.2-1": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.2-3": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.2-6": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.2-4": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.0-BETA": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2: org.apache.spark.sql.catalyst.expressions.Expression apply()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry: org.apache.hadoop.hive.ql.exec.FunctionInfo getFunctionInfo(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$3$$anonfun$apply$2: scala.Some apply()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ],
  "1.6.2-2": [
    "<org.apache.spark.sql.hive.client.ClientWrapper: void overrideHadoopShims()>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$16: scala.collection.mutable.Buffer apply(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.hive.client.HivePartition)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext: void <init>(org.apache.spark.SparkContext)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toQlTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$$anonfun$nodeToPlan$1: java.lang.Object applyOrElse(org.apache.hadoop.hive.ql.parse.ASTNode,scala.Function1)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.expressions.Expression nodeToExpr(org.apache.hadoop.hive.ql.lib.Node)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.parse.ASTNode getAst(java.lang.String,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter org$apache$spark$sql$hive$SparkHiveDynamicPartitionWriterContainer$$newWriter$1(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.conf.HiveConf org$apache$spark$sql$hive$HiveQl$$hiveConf()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.ResolveHiveWindowFunction$$anonfun$apply$1$$anonfun$applyOrElse$1: java.lang.Object applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.hive.ql.Context hiveContext$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.collection.Seq parseDdl(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveContext: org.apache.spark.sql.hive.client.ClientInterface metadataHive$lzycompute()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.hadoop.hive.ql.Context createContext()>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HiveQl$: scala.Tuple2 nodesToGenerator(scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HiveContext$: scala.collection.immutable.Map newTemporaryConfiguration()>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$ClientWrapper$$toViewTable(org.apache.spark.sql.hive.client.HiveTable)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$getFunctionInfo$1: org.apache.hadoop.hive.ql.exec.FunctionInfo apply()>",
    "<org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.client.ClientWrapper: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveQl$: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan createPlan(java.lang.String)>"
  ]
}