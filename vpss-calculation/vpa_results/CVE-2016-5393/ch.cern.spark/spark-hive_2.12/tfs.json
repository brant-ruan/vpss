{
  "3.0.1": [
    "<org.apache.spark.sql.hive.execution.HiveTableScanExec: org.apache.hadoop.hive.ql.plan.TableDesc tableDesc$lzycompute()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.session.SessionState newState()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.execution.HiveScriptIOSchema: scala.Tuple2 $anonfun$initInputSerDe$1(org.apache.spark.sql.hive.execution.HiveScriptIOSchema,scala.collection.Seq,java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveInspectors: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(org.apache.spark.sql.types.DataType)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.HadoopTableReader: org.apache.spark.rdd.RDD $anonfun$makeRDDForPartitionedTable$6(org.apache.spark.sql.hive.HadoopTableReader,scala.Option,scala.Tuple2)>",
    "<org.apache.spark.sql.hive.security.HiveDelegationTokenProvider: org.apache.hadoop.conf.Configuration hiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveDirCommand: scala.collection.Seq run(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.SparkPlan)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.Tuple2 $anonfun$formatTimeVarsForHiveClient$2(org.apache.hadoop.conf.Configuration,scala.Tuple2)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat: scala.collection.Iterator $anonfun$buildReader$2(org.apache.spark.broadcast.Broadcast,boolean,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.execution.SaveAsHiveFile: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$: scala.collection.Iterator fillObject(scala.collection.Iterator,org.apache.hadoop.hive.serde2.Deserializer,scala.collection.Seq,org.apache.spark.sql.catalyst.InternalRow,org.apache.hadoop.hive.serde2.Deserializer)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$: void initializeLocalJobConfFunc(java.lang.String,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive client()>",
    "<org.apache.spark.sql.hive.execution.HiveTableScanExec: void addColumnMetadataToConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Table toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option)>",
    "<org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1: java.lang.String getFileExtension(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.spark.sql.hive.security.HiveDelegationTokenProvider: void $anonfun$obtainDelegationTokens$8()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void closeDriver$1(org.apache.hadoop.hive.ql.Driver)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.spark.sql.hive.security.HiveDelegationTokenProvider: void $anonfun$obtainDelegationTokens$4(org.apache.spark.sql.hive.security.HiveDelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.security.Credentials)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.metastore.api.FieldSchema toHiveColumn(org.apache.spark.sql.types.StructField)>",
    "<org.apache.spark.sql.hive.execution.HiveOutputWriter: void <init>(java.lang.String,org.apache.spark.sql.hive.HiveShim$ShimFileSinkDesc,org.apache.hadoop.mapred.JobConf,org.apache.spark.sql.types.StructType)>",
    "<org.apache.spark.sql.hive.execution.HiveFileFormat: org.apache.spark.sql.execution.datasources.OutputWriterFactory prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.ql.metadata.Partition toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$: scala.Tuple2 $anonfun$getFileReader$3(org.apache.hadoop.fs.FileSystem,boolean,org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$: org.apache.hadoop.hive.conf.HiveConf newHiveConf(org.apache.spark.SparkConf,java.lang.Iterable,scala.collection.immutable.Map,scala.Option)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: scala.collection.Seq run(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.SparkPlan)>",
    "<org.apache.spark.sql.hive.HiveUtils$: org.apache.spark.sql.catalyst.catalog.CatalogTable inferSchema(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.Tuple2 $anonfun$formatTimeVarsForHiveClient$1(org.apache.hadoop.conf.Configuration,scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: java.lang.Object $anonfun$withHiveState$1(org.apache.spark.sql.hive.client.HiveClientImpl,scala.Function0)>",
    "<org.apache.spark.sql.hive.execution.SaveAsHiveFile: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.spark.sql.hive.HadoopTableReader: boolean $anonfun$makeRDDForPartitionedTable$2(org.apache.spark.sql.hive.HadoopTableReader,scala.collection.mutable.Set,scala.collection.mutable.Set,scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ]
}