{
  "2.3.3": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ],
  "2.3.2": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ],
  "2.3.5": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ],
  "2.3.4": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ],
  "2.3.1": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ],
  "2.3.6": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ],
  "2.3.0": [
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalTimeDeserializer: org.joda.time.LocalTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.com.metamx.common.config.DurationCoercible$1: org.joda.time.Duration coerce(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <init>(org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidSchema,java.lang.String,org.apache.hive.druid.org.apache.calcite.rel.type.RelProtoDataType,java.util.Set,java.lang.String,java.util.List)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.com.metamx.emitter.service.AlertEvent$Severity,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CustomVersioningPolicy: void <init>(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.NoopRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceMetrics(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getReverseIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.Interval getInterval()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.MessageTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: java.lang.Iterable getIterable(org.joda.time.DateTime,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File makeIndexFiles(java.util.List,org.apache.hive.druid.io.druid.query.aggregation.AggregatorFactory[],java.io.File,org.apache.hive.druid.io.druid.segment.ProgressIndicator,java.util.List,java.util.List,org.apache.hive.druid.com.google.common.base.Function,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat withFormat(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: void <init>(org.joda.time.Period,org.joda.time.DateTime,org.joda.time.DateTimeZone)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: org.joda.time.Period getChunkPeriod(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.MonthDayDeserializer: org.joda.time.MonthDay deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.audit.SQLAuditManager: org.joda.time.Interval getIntervalOrDefault(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.guice.http.DruidHttpClientConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.RealtimeTuningConfig: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTableFactory: org.apache.hive.druid.org.apache.calcite.schema.Table create(org.apache.hive.druid.org.apache.calcite.schema.SchemaPlus,java.lang.String,java.util.Map,org.apache.hive.druid.org.apache.calcite.rel.type.RelDataType)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifier(long)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void resetNextFlush()>",
    "<org.apache.hive.druid.io.druid.server.initialization.ServerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] getInputSplits(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator$CoordinatorHistoricalManagerRunnable$1: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexMergerV9: void makeIndexBinary(org.apache.hive.druid.com.metamx.common.io.smoosh.FileSmoosher,java.util.List,java.io.File,java.util.List,java.util.List,org.apache.hive.druid.io.druid.segment.ProgressIndicator,org.apache.hive.druid.io.druid.segment.IndexSpec,java.util.List)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: java.util.Map getSimpleDatasource(java.lang.String)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] distributeSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$ReverseIntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.client.CachingClusteredClient: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getQueryTargets(java.lang.String,java.lang.String,int,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void mergeAndPush()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.spec.LegacySegmentSpec$1: org.joda.time.Interval apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean announceHistoricalSegment(org.skife.jdbi.v2.Handle,org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.timeline.DataSegmentUtils: org.apache.hive.druid.io.druid.timeline.DataSegmentUtils$SegmentIdentifierParts parse(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$4: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureOutputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$V9IndexLoader: org.apache.hive.druid.io.druid.segment.QueryableIndex load(java.io.File,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims: java.lang.String toString()>",
    "<org.apache.hadoop.hive.druid.io.DruidRecordWriter: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier getSegmentIdentifierAndMaybePush(long)>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet: void service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: java.lang.Object bootstrapSinksFromDisk()>",
    "<org.apache.hive.druid.io.druid.server.log.FileRequestLogger: void start()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex$TimeAndDims toTimeAndDims(org.apache.hive.druid.io.druid.data.input.InputRow)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List createSplitsIntervals(java.util.List,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.server.metrics.DruidMonitorSchedulerConfig: void <init>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateTimeDeserializer: org.joda.time.LocalDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.coordination.BatchDataSegmentAnnouncer: java.lang.String makeServedSegmentPath()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IngestSegmentFirehose$1$1$1$1: org.apache.hive.druid.io.druid.data.input.InputRow next()>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response getSegmentDataSourceSpecificInterval(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void flushAfterDuration(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$7: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.query.IntervalChunkingQueryRunner: java.lang.Iterable splitInterval(org.joda.time.Interval,org.joda.time.Period)>",
    "<org.apache.hive.druid.io.druid.query.extraction.TimeFormatExtractionFn: void <init>(java.lang.String,org.joda.time.DateTimeZone,java.lang.String,org.apache.hive.druid.io.druid.granularity.QueryGranularity)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinator: java.util.Map getReplicationStatus()>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.LocalDateDeserializer: org.joda.time.LocalDate deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidDateTimeUtils$1: org.joda.time.Interval apply(org.apache.hive.druid.com.google.common.collect.Range)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource: javax.ws.rs.core.Response doPost(java.io.InputStream,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataSegmentManagerConfig: void <init>()>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: java.util.Map buildStringKeyMap(java.nio.ByteBuffer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.Rowboat: java.lang.String toString()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: void <init>(java.lang.String,int)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.com.metamx.http.client.HttpClient makeHttpClient(org.apache.hive.druid.com.metamx.common.lifecycle.Lifecycle)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentPublisher: void publishSegment(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.indexer.SQLMetadataStorageUpdaterJobHandler$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.io.druid.server.initialization.HttpEmitterConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval widen(org.joda.time.Interval)>",
    "<org.apache.hive.druid.io.druid.server.QueryResource$2: void write(java.io.OutputStream)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator: boolean updateDataSourceMetadataWithHandle(org.skife.jdbi.v2.Handle,java.lang.String,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata,org.apache.hive.druid.io.druid.indexing.overlord.DataSourceMetadata)>",
    "<org.apache.hive.druid.io.druid.common.utils.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose: javax.ws.rs.core.Response shutdown(java.lang.String)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidTable: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorRuleRunner: org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams run(org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.query.select.SelectQueryEngine$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.io.druid.query.groupby.strategy.GroupByStrategyV2: org.joda.time.DateTime getUniversalTimestamp(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataRuleManager$4: java.lang.Void inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.org.apache.calcite.adapter.druid.DruidQuery: boolean isValid(org.apache.hive.druid.org.apache.calcite.util.Litmus)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$6: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator: org.apache.hive.druid.io.druid.query.QueryRunner postProcess(org.apache.hive.druid.io.druid.query.QueryRunner)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.firehose.IrcFirehoseFactory$1: void onChannelMessage(com.ircclouds.irc.api.domain.messages.ChannelPrivMsg)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.key.DateTimeKeyDeserializer: org.joda.time.DateTime deserialize(java.lang.String,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeZoneDeserializer: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.InstantDeserializer: org.joda.time.Instant deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.router.TieredBrokerHostSelector: org.apache.hive.druid.com.metamx.common.Pair select(org.apache.hive.druid.io.druid.query.Query)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: void poll()>",
    "<org.apache.hive.druid.io.druid.segment.indexing.granularity.ArbitraryGranularitySpec: org.apache.hive.druid.com.google.common.base.Optional bucketInterval(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$1: org.joda.time.DateTime apply(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.http.RulesResource: java.util.List getRuleHistory(java.lang.String,java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.ServiceMetricEvent: void <init>(org.joda.time.DateTime,java.lang.String,java.lang.String,java.util.Map,java.lang.String,java.lang.Number)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onFailure(org.eclipse.jetty.client.api.Response,java.lang.Throwable)>",
    "<org.apache.hive.druid.io.druid.metadata.MetadataRuleManagerConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.FlushingPlumber: void startFlushThread()>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMaxTime()>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$DateTimeDeserializer: org.joda.time.DateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.server.listener.announcer.ListenerResourceAnnouncer: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.segment.IndexMerger: java.io.File persist(org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex,org.joda.time.Interval,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec,org.apache.hive.druid.io.druid.segment.ProgressIndicator)>",
    "<org.apache.hive.druid.io.druid.segment.QueryableIndexStorageAdapter: org.apache.hive.druid.com.metamx.common.guava.Sequence makeCursors(org.apache.hive.druid.io.druid.query.filter.Filter,org.joda.time.Interval,org.apache.hive.druid.io.druid.granularity.QueryGranularity,boolean)>",
    "<org.apache.hive.druid.io.druid.query.Druids$ResultBuilder: void <init>()>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSupervisorManager$1: java.lang.Void withHandle(org.skife.jdbi.v2.Handle)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.PeriodDeserializer: org.joda.time.Period deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$3: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.DruidCoordinatorRuntimeParams$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: void persistAndMerge(long,org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: org.joda.time.Interval umbrellaInterval(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryRunnerFactory$TimeBoundaryQueryRunner$1: org.apache.hive.druid.io.druid.query.Result apply(org.apache.hive.druid.io.druid.segment.Cursor)>",
    "<org.apache.hive.druid.com.metamx.common.JodaUtils: java.util.ArrayList condenseIntervals(java.lang.Iterable)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager$9$1: org.joda.time.Interval mapInternal(int,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.SegmentAnalyzer: org.apache.hive.druid.io.druid.query.metadata.metadata.ColumnAnalysis analyzeStringColumn(org.apache.hive.druid.io.druid.segment.column.ColumnCapabilities,org.apache.hive.druid.io.druid.segment.StorageAdapter,java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentKiller: org.joda.time.Interval findIntervalForKillTask(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$10: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry$Builder: void <init>()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: org.joda.time.Interval getMergedTimelineInterval()>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.util.Map getDatasource(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser: org.apache.hive.druid.com.google.common.base.Function createTimestampParser(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: boolean addAtKey(java.util.NavigableMap,org.joda.time.Interval,org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline$TimelineEntry)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1: org.apache.hive.druid.com.metamx.common.guava.Sequence run(org.apache.hive.druid.io.druid.query.Query,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifierConfig: void <init>()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getUniqueId()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.data.input.ProtoBufInputRowParser: com.google.protobuf.Descriptors$Descriptor getDescriptor(java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$9: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$7: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: void convertV8toV9(java.io.File,java.io.File,org.apache.hive.druid.io.druid.segment.IndexSpec)>",
    "<org.apache.hive.druid.io.druid.server.http.IntervalsResource: javax.ws.rs.core.Response getSpecificIntervals(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$1: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.RealtimePlumber: org.apache.hive.druid.io.druid.segment.realtime.plumber.Sink getSink(long)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.YearMonthDeserializer: org.joda.time.YearMonth deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.PeriodGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler: java.lang.String getRootWorkingDir()>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: void <init>(org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat,java.util.TimeZone)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity: org.joda.time.Interval bucket(org.joda.time.DateTime)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: java.lang.Iterable getDatasourceDimensions(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.metamx.emitter.service.AlertEvent: void <init>(java.lang.String,java.lang.String,java.lang.String,java.util.Map)>",
    "<org.apache.hive.druid.io.druid.query.metadata.metadata.SegmentMetadataQuery: void <clinit>()>",
    "<org.apache.hive.druid.io.druid.server.http.DatasourcesResource: javax.ws.rs.core.Response deleteDataSourceSpecificInterval(java.lang.String,java.lang.String)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateMidnightDeserializer: org.joda.time.DateMidnight deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.metadata.IndexerSQLMetadataStorageCoordinator$3: org.apache.hive.druid.io.druid.segment.realtime.appenderator.SegmentIdentifier inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.ser.JacksonJodaFormat: org.joda.time.format.DateTimeFormatter createFormatter(org.apache.hive.druid.com.fasterxml.jackson.databind.SerializerProvider)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.plumber.ServerTimeRejectionPolicyFactory$1: org.joda.time.DateTime getCurrMaxTime()>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedRow: void <init>(long,java.util.Map)>",
    "<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.IndexIO$DefaultIndexIOHandler: org.apache.hive.druid.io.druid.segment.MMappedIndex mapDir(java.io.File)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hive.druid.io.druid.server.http.CoordinatorDynamicConfigsResource: javax.ws.rs.core.Response getDatasourceRuleHistory(java.lang.String,java.lang.Integer)>",
    "<org.apache.hive.druid.io.druid.jackson.JodaStuff$IntervalDeserializer: org.joda.time.Interval deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.jackson.DruidDefaultSerializersModule$2: org.joda.time.DateTimeZone deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.data.input.MapBasedInputRow: java.lang.String toString()>",
    "<org.apache.hive.druid.io.druid.query.timeboundary.TimeBoundaryQueryQueryToolChest$5$2: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.granularity.DurationGranularity: org.joda.time.DateTime getOrigin()>",
    "<org.apache.hive.druid.io.druid.server.coordinator.helper.DruidCoordinatorSegmentMerger$SegmentsToMerge: boolean add(org.apache.hive.druid.io.druid.timeline.TimelineObjectHolder)>",
    "<org.apache.hive.druid.io.druid.query.TimewarpOperator$1$1: java.lang.Object apply(java.lang.Object)>",
    "<org.apache.hive.druid.com.metamx.common.parsers.TimestampParser$8: org.joda.time.DateTime apply(java.lang.Number)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$11: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$6: org.joda.time.DateTime toDate(java.lang.String,org.apache.hive.druid.com.metamx.common.Granularity$Formatter)>",
    "<org.apache.hadoop.hive.druid.DruidStorageHandler$4: java.net.URL apply(org.apache.hive.druid.io.druid.timeline.DataSegment)>",
    "<org.apache.hive.druid.io.druid.segment.incremental.IncrementalIndex: org.joda.time.DateTime getMinTime()>",
    "<org.apache.hive.druid.io.druid.common.config.ConfigManagerConfig: void <init>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$8: int numIn(org.joda.time.ReadableInterval)>",
    "<org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorPlumber: void startPersistThread()>",
    "<org.apache.hive.druid.io.druid.query.search.SearchQueryQueryToolChest$4$3: org.apache.hive.druid.io.druid.query.Result apply(java.lang.Object)>",
    "<org.apache.hive.druid.io.druid.timeline.VersionedIntervalTimeline: java.util.List lookup(org.joda.time.Interval,boolean)>",
    "<org.apache.hive.druid.io.druid.granularity.QueryGranularities: void <clinit>()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$IntervalIterator: org.joda.time.Interval next()>",
    "<org.apache.hive.druid.io.druid.server.AsyncQueryForwardingServlet$MetricsEmittingProxyResponseListener: void onComplete(org.eclipse.jetty.client.api.Result)>",
    "<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: org.apache.hadoop.hive.druid.io.HiveDruidSplit[] splitSelectQuery(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hive.druid.io.druid.query.select.SelectQuery,org.apache.hadoop.fs.Path)>",
    "<org.apache.hive.druid.io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2: org.apache.hive.druid.com.metamx.common.guava.Sequence process(org.apache.hive.druid.io.druid.query.groupby.GroupByQuery,org.apache.hive.druid.io.druid.segment.StorageAdapter,org.apache.hive.druid.io.druid.collections.StupidPool,org.apache.hive.druid.io.druid.query.groupby.GroupByQueryConfig)>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$2: void <init>(java.lang.String,int)>",
    "<org.apache.hive.druid.io.druid.audit.AuditEntry: void <init>(java.lang.String,java.lang.String,org.apache.hive.druid.io.druid.audit.AuditInfo,java.lang.String,org.joda.time.DateTime)>",
    "<org.apache.hive.druid.com.fasterxml.jackson.datatype.joda.deser.DateTimeDeserializer: org.joda.time.ReadableDateTime deserialize(org.apache.hive.druid.com.fasterxml.jackson.core.JsonParser,org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext)>",
    "<org.apache.hive.druid.io.druid.granularity.BaseQueryGranularity: org.joda.time.DateTime toDateTime(long)>",
    "<org.apache.hive.druid.io.druid.query.datasourcemetadata.DataSourceMetadataQuery: java.lang.Iterable mergeResults(java.util.List)>",
    "<org.apache.hive.druid.io.druid.metadata.SQLMetadataSegmentManager: boolean enableDatasource(java.lang.String)>",
    "<org.apache.hive.druid.io.druid.server.ClientInfoResource: org.joda.time.DateTime getCurrentTime()>",
    "<org.apache.hive.druid.com.metamx.common.Granularity$5: int numIn(org.joda.time.ReadableInterval)>"
  ]
}