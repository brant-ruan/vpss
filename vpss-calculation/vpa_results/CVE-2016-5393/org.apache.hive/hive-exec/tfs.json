{
  "2.3.3": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.2": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.5": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.4": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.1": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.7": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.6": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.0": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB createProxy()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.lang.String getSargColumnIDsString(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: boolean getBoolVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.spark.client.SparkClientImpl: java.lang.Thread startDriver(org.apache.hive.spark.client.rpc.RpcServer,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics: void initReporting(java.util.Set)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.Set getNestedColumnPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: boolean isReadAllColumns(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.llap.LlapUtil: void loginWithKerberosAndUpdateCurrentUser(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.llap.LlapUtil: org.apache.hadoop.security.UserGroupInformation loginWithKerberos(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void startThreads()>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.util.Set getJarFilesByPath(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,int,org.apache.hadoop.io.retry.RetryPolicy,javax.net.SocketFactory)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB createProxyInternal()>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageScheme(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.LlapSignerImpl: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager createSecretManager(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite: void <init>(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hive.common.util.ACLConfigurationParser: void parse(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: void init(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void <init>(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.HiveMetaHookLoader,java.lang.Boolean)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean areOptimizationsEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapTokenLocalClient getLocalTokenClient(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl$FixedServiceInstance: void <init>(org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl,java.lang.String)>",
    "<org.apache.hive.http.HttpServer: void setupSpnegoFilter(org.apache.hive.http.HttpServer$Builder)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.BlobStorageUtils: boolean isBlobStorageAsScratchDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.coordinator.LlapCoordinator: org.apache.hadoop.hive.llap.security.LlapSigner getLlapSigner(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getChksumString(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.security.token.Token createLlapToken(java.lang.String,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol,int,java.lang.String,org.apache.hadoop.security.token.Token)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener: boolean isReadAuthzEnabled()>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: org.apache.hadoop.hive.llap.security.SecretManager$LlapZkConf createLlapZkConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.llap.security.SecretManager: void verifyToken(byte[])>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "1.2.0": [
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$6: void interrupt()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showColumns(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowColumnsDesc)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: java.lang.String downloadResource(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void applyConstraints(java.net.URI,java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeFunction(org.apache.hadoop.hive.ql.plan.DescFunctionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.orc.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$ReaderOptions)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showFunctions(org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniTezCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.DriverContext,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.ql.io.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCreateTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc)>",
    "<parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int descDatabase(org.apache.hadoop.hive.ql.plan.DescDatabaseDesc)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,int,java.util.Properties,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.ArrayList)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showIndexes(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowIndexesDesc)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<parquet.hadoop.ParquetFileReader: parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTables(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.exec.Task)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showDatabases(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTxns(org.apache.hadoop.hive.ql.plan.ShowTxnsDesc)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTableStatus(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniSparkCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void replaceFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: java.lang.String getDelegationToken(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<parquet.hadoop.ParquetFileReader: parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropSessionPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printJsonData(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$7: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.ql.session.SessionState start(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCompactions(org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocks(org.apache.hadoop.hive.ql.plan.ShowLocksDesc)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DescTableDesc)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocksNewFormat(org.apache.hadoop.hive.ql.plan.ShowLocksDesc,org.apache.hadoop.hive.ql.lockmgr.HiveLockManager)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showPartitions(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.0.0": [
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: java.lang.String downloadResource(java.lang.String,boolean)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.orc.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$ReaderOptions)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,boolean)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: org.apache.hadoop.hive.ql.io.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,int,java.util.Properties,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC()>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void replaceFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: java.lang.String getDelegationToken(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropSessionPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.io.orc.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.1.0": [
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl$1: java.util.List getAclForPath(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,int,java.util.Properties,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.CreateTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(java.util.List,org.apache.hadoop.hive.ql.plan.CreateTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropSessionPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl: boolean deleteDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.2.1": [
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$6: void interrupt()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showColumns(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowColumnsDesc)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: java.lang.String downloadResource(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void applyConstraints(java.net.URI,java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeFunction(org.apache.hadoop.hive.ql.plan.DescFunctionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.orc.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$ReaderOptions)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showFunctions(org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniTezCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.DriverContext,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.ql.io.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCreateTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc)>",
    "<parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int descDatabase(org.apache.hadoop.hive.ql.plan.DescDatabaseDesc)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,int,java.util.Properties,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.ArrayList)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showIndexes(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowIndexesDesc)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<parquet.hadoop.ParquetFileReader: parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTables(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.exec.Task)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showDatabases(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTxns(org.apache.hadoop.hive.ql.plan.ShowTxnsDesc)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTableStatus(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: org.apache.hadoop.hive.shims.Hadoop20SShims$MiniMrShim getMiniSparkCluster(org.apache.hadoop.conf.Configuration,int,java.lang.String,int)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void replaceFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: java.lang.String getDelegationToken(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<parquet.hadoop.ParquetFileReader: parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropSessionPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.orc.FileDump: void printJsonData(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$7: void run()>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.shims.Hadoop20SShims: boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.ql.session.SessionState start(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showCompactions(org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocks(org.apache.hadoop.hive.ql.plan.ShowLocksDesc)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int describeTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DescTableDesc)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showLocksNewFormat(org.apache.hadoop.hive.ql.plan.ShowLocksDesc,org.apache.hadoop.hive.ql.lockmgr.HiveLockManager)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showPartitions(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc)>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.1.1": [
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void checkAndSetAcls()>",
    "<org.apache.hadoop.hive.serde2.ColumnProjectionUtils: java.util.List getReadColumnIDs(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void setupZookeeperAuth(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.common.ServerUtils: void cleanUpScratchDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.common.FileUtils: void checkFileAccessWithImpersonation(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode genValuesTempTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB)>",
    "<org.apache.hive.http.HttpServer: boolean userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.conf.HiveConf: java.lang.String[] getTrimmedStringsVar(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf$ConfVars)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.spark.client.SparkClientUtilities: java.net.URL urlFromPathString(java.lang.String,java.lang.Long,org.apache.hadoop.conf.Configuration,java.io.File)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForLocation(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$JsonFileReporter$1: void run()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy: org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB getProxy(org.apache.hadoop.hive.llap.LlapNodeId)>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.llap.security.LlapTokenClient: org.apache.hadoop.security.token.Token getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getFileLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.common.FileUtils: java.io.File createLocalDirsTempFile(java.lang.String,java.lang.String,java.lang.String,boolean)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl: void <init>(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.CreateTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: java.util.List add_partitions_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.common.FileUtils: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getFileSizeForPartitions(org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(java.util.List,org.apache.hadoop.hive.ql.plan.CreateTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>",
    "<org.apache.hive.http.HttpServer$Builder: org.apache.hive.http.HttpServer$Builder setAdmins(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setWorkflowAdjacencies(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.QueryPlan)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void checkTrashPurgeCombination(org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: int add_partitions_pspec_core(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String,java.util.List,boolean)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: boolean hasMetastoreBasedSchema(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileSystem getFs(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.metastore.TUGIBasedProcessor: void handleSetUGI(org.apache.hadoop.hive.thrift.TUGIContainingTransport,org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_ugi,org.apache.thrift.protocol.TMessage,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.common.FileUtils$3: java.lang.Object run()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.io.SymbolicInputFormat: void rework(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.MapredWork)>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.metastore.FileMetadataManager: void cacheMetadata(org.apache.hadoop.hive.metastore.api.FileMetadataExprType,java.lang.String)>",
    "<org.apache.hadoop.hive.common.FileUtils: boolean isLocalFile(org.apache.hadoop.hive.conf.HiveConf,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: void reloginExpiringKeytabUser()>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.metastore.Warehouse: org.apache.hadoop.fs.FileStatus[] getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl: boolean deleteDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>"
  ]
}