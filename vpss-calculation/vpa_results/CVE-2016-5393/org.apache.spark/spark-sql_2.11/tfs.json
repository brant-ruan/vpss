{
  "2.3.3": [
    "<org.apache.spark.sql.execution.datasources.orc.OrcUtils$: scala.Option readSchema(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>"
  ],
  "2.3.2": [
    "<org.apache.spark.sql.execution.datasources.orc.OrcUtils$: scala.Option readSchema(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>"
  ],
  "2.3.4": [
    "<org.apache.spark.sql.execution.datasources.orc.OrcUtils$: scala.Option readSchema(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>"
  ],
  "2.3.1": [
    "<org.apache.spark.sql.execution.datasources.orc.OrcUtils$: scala.Option readSchema(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>"
  ],
  "2.3.0": [
    "<org.apache.spark.sql.execution.datasources.orc.OrcUtils$: scala.Option readSchema(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>"
  ]
}