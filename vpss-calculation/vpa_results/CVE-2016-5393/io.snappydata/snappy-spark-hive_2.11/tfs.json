{
  "2.1.1.4": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.3-2": [
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ],
  "2.1.1.2": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.1.1.8": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSessionImpl$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.1-3": [
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ],
  "2.0.2.5": [
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ],
  "2.1.1.1-rc1": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.1.1.3-RC1": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.2.3": [
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ],
  "2.1.1.6": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.1-2": [
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ],
  "2.1.1.3": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.0.1-1": [
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>"
  ],
  "2.1.1.7": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSessionImpl$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.1.1.5": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ],
  "2.1.1.1": [
    "<org.apache.spark.sql.hive.HiveSimpleUDF: org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper conversionHelper$lzycompute()>",
    "<org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader: void <init>(org.apache.hadoop.hive.ql.io.orc.Reader,org.apache.hadoop.conf.Configuration,long,long)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Hive org$apache$spark$sql$hive$client$HiveClientImpl$$client()>",
    "<org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter newOutputWriter$1(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)>",
    "<org.apache.spark.sql.hive.HiveSimpleUDF: java.lang.Object eval(org.apache.spark.sql.catalyst.InternalRow)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Partition org$apache$spark$sql$hive$client$HiveClientImpl$$toHivePartition(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1: java.lang.Object apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2: scala.Tuple2 apply(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$verifyPartitionPath$1$1: boolean apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5: org.apache.spark.rdd.RDD apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: org.apache.hadoop.hive.ql.metadata.Table org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)>",
    "<org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$reset$3: void apply(java.lang.String)>",
    "<org.apache.spark.sql.hive.HiveSessionCatalog: org.apache.spark.sql.catalyst.expressions.Expression lookupFunction0(org.apache.spark.sql.catalyst.FunctionIdentifier,scala.collection.Seq)>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1: scala.collection.Seq apply()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl: void <init>(org.apache.spark.sql.hive.client.package$HiveVersion,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,scala.collection.immutable.Map,java.lang.ClassLoader,org.apache.spark.sql.hive.client.IsolatedClientLoader)>",
    "<org.apache.spark.sql.hive.HiveUtils$$anonfun$hiveClientConfigurations$1: scala.Tuple2 apply(scala.Tuple2)>",
    "<org.apache.spark.sql.hive.execution.InsertIntoHiveTable: org.apache.hadoop.fs.Path oldVersionExternalTempPath(org.apache.hadoop.fs.Path)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: java.lang.String getOutputName()>",
    "<org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1: java.lang.Object apply()>",
    "<org.apache.spark.sql.hive.MetastoreRelation$$anonfun$getHiveQlPartitions$1: org.apache.hadoop.hive.ql.metadata.Partition apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)>",
    "<org.apache.spark.sql.hive.HiveTableUtil$: void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2: scala.collection.Iterator apply(org.apache.spark.sql.execution.datasources.PartitionedFile)>",
    "<org.apache.spark.sql.hive.HiveUtils$: scala.collection.immutable.Map newTemporaryConfiguration(boolean)>",
    "<org.apache.spark.sql.hive.SparkHiveWriterContainer: void initWriters()>"
  ]
}