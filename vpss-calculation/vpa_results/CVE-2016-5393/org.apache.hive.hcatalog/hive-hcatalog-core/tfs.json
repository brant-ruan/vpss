{
  "2.3.4": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.3.5": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.3.6": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "1.2.1": [
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorizeDDL(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.security.authorization.Privilege[],org.apache.hadoop.hive.ql.security.authorization.Privilege[])>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.metadata.Partition,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void postAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: void postAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>"
  ],
  "2.3.1": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.3.3": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.3.2": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.3.7": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.1.0": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.0.0": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: void setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.1.1": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ],
  "2.3.10": [
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>"
  ],
  "1.2.0": [
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorizeDDL(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.security.authorization.Privilege[],org.apache.hadoop.hive.ql.security.authorization.Privilege[])>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.metadata.Partition,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void postAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: void postAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,java.util.List)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase: void authorize(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.security.authorization.Privilege)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>"
  ],
  "2.0.1": [
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: void setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>"
  ],
  "2.3.8": [
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>"
  ],
  "2.3.9": [
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>"
  ],
  "2.3.0": [
    "<org.apache.hive.hcatalog.common.HiveClientCache$HiveClientCacheKey: void <init>(org.apache.hadoop.hive.conf.HiveConf,int)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(org.apache.hadoop.mapreduce.JobID,boolean,int,int)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: void <init>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: java.util.Iterator read()>",
    "<org.apache.hive.hcatalog.data.schema.HCatSchemaUtils: org.apache.hadoop.hive.metastore.api.FieldSchema getFieldSchema(org.apache.hive.hcatalog.data.schema.HCatFieldSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: java.util.Properties getSerdeProperties(org.apache.hive.hcatalog.mapreduce.HCatTableInfo,org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void registerPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook: org.apache.hadoop.hive.ql.parse.ASTNode preAnalyze(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil: boolean isAuthorizationEnabled(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>",
    "<org.apache.hive.hcatalog.mapreduce.Security: void handleSecurity(org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer: void authorizeDDLWork(org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DDLWork)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void <clinit>()>",
    "<org.apache.hive.hcatalog.mapreduce.InternalUtil: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector createStructObjectInspector(org.apache.hive.hcatalog.data.schema.HCatSchema)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: void setColumnNumber(org.apache.hadoop.conf.Configuration,int)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: int processCmd(java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: boolean isHadoop23()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: void handleDuplicatePublish(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader: org.apache.hive.hcatalog.data.transfer.ReaderContext prepareRead()>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.JobContext getJobContext(java.lang.String,org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache$5: org.apache.hive.hcatalog.common.HiveClientCache$ICacheableMetaStoreClient call()>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.Map getColValsNotInDataColumns(org.apache.hive.hcatalog.data.schema.HCatSchema,org.apache.hive.hcatalog.mapreduce.PartInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void discoverPartitions(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: int setFSPermsNGrp(org.apache.hadoop.hive.ql.session.SessionState)>",
    "<org.apache.hive.hcatalog.mapreduce.FosterStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>",
    "<org.apache.hive.hcatalog.cli.HCatCli: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.conf.HiveConf getHiveConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor: void makeHar(org.apache.hadoop.mapreduce.JobContext,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void commitJob(org.apache.hadoop.mapreduce.JobContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void write(java.util.Iterator)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat: java.util.List setInputPath(org.apache.hadoop.mapred.JobConf,java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: org.apache.hadoop.mapreduce.TaskAttemptContext getTaskAttemptContext(java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: org.apache.hive.hcatalog.data.transfer.WriterContext prepareWrite()>",
    "<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.hive.metastore.api.Partition constructPartition(org.apache.hadoop.mapreduce.JobContext,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.String,java.lang.String,java.util.Map,org.apache.hive.hcatalog.data.schema.HCatSchema,java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: void <init>(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer: org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer$LocalFileWriter getLocalFileWriter(org.apache.hive.hcatalog.data.HCatRecord)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatOutputFormat: void setOutput(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: java.util.Map getInputJobProperties(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hive.hcatalog.mapreduce.InputJobInfo)>",
    "<org.apache.hive.hcatalog.cli.HCatDriver: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse run(java.lang.String)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>",
    "<org.apache.hive.hcatalog.mapreduce.SpecialCases: void addSpecialCasesParametersToOutputJobProperties(java.util.Map,org.apache.hive.hcatalog.mapreduce.OutputJobInfo,java.lang.Class)>",
    "<org.apache.hive.hcatalog.common.HiveClientCache: org.apache.hadoop.hive.metastore.IMetaStoreClient getNonCachedHiveMetastoreClient(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.mapreduce.HCatTableInfo: org.apache.hive.hcatalog.data.schema.HCatSchema getAllColumns()>",
    "<org.apache.hive.hcatalog.common.HCatUtil: void configureOutputStorageHandler(org.apache.hadoop.hive.ql.metadata.HiveStorageHandler,org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.OutputJobInfo)>",
    "<org.apache.hive.hcatalog.mapreduce.InitializeInput: org.apache.hive.hcatalog.mapreduce.InputJobInfo getInputJobInfo(org.apache.hadoop.conf.Configuration,org.apache.hive.hcatalog.mapreduce.InputJobInfo,java.lang.String)>",
    "<org.apache.hive.hcatalog.common.HCatUtil: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler getStorageHandler(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat: org.apache.hadoop.mapreduce.RecordWriter getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void abort(org.apache.hive.hcatalog.data.transfer.WriterContext)>",
    "<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat$MultiRecordWriter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hive.hcatalog.oozie.JavaAction: void main(java.lang.String[])>",
    "<org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil: org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter: void commit(org.apache.hive.hcatalog.data.transfer.WriterContext)>"
  ]
}