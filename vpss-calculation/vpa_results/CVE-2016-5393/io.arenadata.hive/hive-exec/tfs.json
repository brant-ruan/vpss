{
  "2.3.9_arenadata2": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.9_arenadata1": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ],
  "2.3.9_arenadata3": [
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.fs.Path dumpDbMetadata(java.lang.String,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.lang.String relativeToAbsolutePath(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.JobConf,java.lang.Class,org.apache.hadoop.mapred.RecordReader)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int unarchive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.exec.MoveTask: void moveFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Utils: void setZookeeperClientKerberosJaasConfig(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.FileStatus validateTargetDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: void cancelDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: void ensureFileFormatsMatch(org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,java.util.List,java.net.URI)>",
    "<org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat: org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path setBaseWork(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.fs.Path,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path getDefaultDestDir(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: void getMergedCredentials(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.index.HiveIndexResult: void <init>(java.util.List,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: void cleanScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.SecureCmdDoAs: void <init>(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims: org.apache.hadoop.fs.FileSystem getNonCachedFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>",
    "<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim getHdfsEncryptionShim()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: boolean isCurrentUserOwner()>",
    "<org.apache.orc.tools.FileDump: void printMetaDataImpl(java.lang.String,org.apache.hadoop.conf.Configuration,java.util.List,boolean,java.util.List)>",
    "<org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$WrapperRecordWriter: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveWriter()>",
    "<org.apache.hadoop.hive.ql.hooks.ATSHook: java.lang.String createOrGetDomain(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void dropPathAndUnregisterDeleteOnExit(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: org.apache.hadoop.mapred.FileSplit getFileSplitFromEvent(org.apache.tez.runtime.api.events.InputDataInformationEvent)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String realFile(java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.impl.HadoopShims$Factory: org.apache.orc.impl.HadoopShims get()>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathReadOnly(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexPartitionFresh(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.metastore.api.Index,org.apache.hadoop.hive.ql.metadata.Partition)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcFile: org.apache.hadoop.hive.ql.io.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean initializeBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,int,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HashTableLoader: void load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[],org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[])>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin: void resolveUnknownSizes(org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin$ConditionalResolverCommonJoinCtx,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server: void <init>()>",
    "<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void analyzeAlterTableLocation(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,java.util.HashMap)>",
    "<org.apache.avro.mapred.SequenceFileReader: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.HiveDelegationTokenManager: java.lang.String getDelegationToken(java.lang.String,java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkTable(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.PartitionIterable,boolean,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker: boolean validateInput(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>",
    "<org.apache.avro.mapred.AvroUtf8InputFormat: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.io.SequenceFile$Writer createSequenceWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void setPlanPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client: org.apache.thrift.transport.TTransport createClientTransport(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.thrift.transport.TTransport,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: org.apache.hadoop.fs.Path createRootHDFSDir(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.fs.Path createTezDir(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Partition: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: void prepareExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.net.URI,org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context,java.util.List,java.util.HashSet,java.util.HashSet,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: boolean isAcid(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: java.util.Collection getAllFilesInPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.FooterBuffer: boolean updateBuffer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path createEmptyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.io.HiveOutputFormat,org.apache.hadoop.mapred.JobConf,java.util.Properties,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void createTmpDirs(org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.FunctionTask: void checkLocalFunctionResources(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema)>",
    "<org.apache.hadoop.hive.ql.Context: void clear()>",
    "<org.apache.hadoop.hive.shims.Utils: java.lang.String getTokenStrForm(java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile$Reader: void <init>(org.apache.avro.hadoop.io.AvroSequenceFile$Reader$Options)>",
    "<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>",
    "<org.apache.avro.mapreduce.AvroOutputFormatBase: java.io.OutputStream getAvroFileOutputStream(org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int msck(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.MsckDesc)>",
    "<org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport: java.util.List getColumnNames(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: boolean getNextPath()>",
    "<org.apache.hadoop.hive.ql.parse.EximUtil: java.net.URI getValidatedURI(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>",
    "<org.apache.avro.hadoop.io.AvroSerialization: void addToConfiguration(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.orc.tools.FileDump: void recoverFiles(java.util.List,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir: void run()>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>",
    "<org.apache.hadoop.hive.ql.io.merge.MergeFileWork: void resolveConcatenateMerge(org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: boolean checkPreExisting(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities$2: void run()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getLocalScratchDir(boolean)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: java.lang.String getServerPrincipal(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>",
    "<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.fs.Path createDummyFile()>",
    "<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.hive.ql.optimizer.SamplePruner$LimitPruneRetStatus limitPrune(org.apache.hadoop.hive.ql.metadata.Partition,long,int,java.util.Collection)>",
    "<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void start(org.apache.hadoop.hive.ql.session.SessionState,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>",
    "<org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor: void run(java.util.Map,java.util.Map)>",
    "<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.metadata.Table materializeCTE(java.lang.String,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$CTEClause)>",
    "<org.apache.hadoop.hive.thrift.ZooKeeperTokenStore: void setupJAASConfig(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: void clearWork(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void dumpEvent(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.Context: void removeMaterializedCTEs()>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getQualifiedPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)>",
    "<org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer: org.apache.hadoop.fs.Path tryQualifyPath(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>",
    "<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void flushToFile(org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer,byte)>",
    "<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownTables(java.lang.String,java.util.List,org.apache.hadoop.hive.ql.metadata.CheckResult)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.io.FlatFileInputFormat$FlatFileRecordReader: void <init>(org.apache.hadoop.hive.ql.io.FlatFileInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)>",
    "<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.metadata.Table: org.apache.hadoop.fs.FileStatus[] getSortedPaths()>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim: void <init>(org.apache.hadoop.hive.shims.Hadoop23Shims,java.net.URI,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: java.util.List getBucketFilePathsOfPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ParseContext)>",
    "<org.apache.orc.OrcFile: org.apache.orc.Writer createWriter(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$WriterOptions)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesCompressor: void <init>(org.apache.parquet.hadoop.metadata.CompressionCodecName,org.apache.hadoop.io.compress.CompressionCodec,int)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Metadata getMetadata(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer$DumpMetaData: void loadDumpFromFile()>",
    "<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getScratchDir(java.lang.String,java.lang.String,boolean,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer: void clear()>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createSessionDirs(java.lang.String)>",
    "<org.apache.avro.mapred.FsInput: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>",
    "<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>",
    "<org.apache.avro.mapred.tether.TetheredProcess: java.lang.Process startSubprocess(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidTxnList,org.apache.hive.common.util.Ref,boolean)>",
    "<org.apache.avro.hadoop.io.AvroSequenceFile: org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.avro.hadoop.io.AvroSequenceFile$Writer$Options)>",
    "<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl: org.apache.hadoop.fs.Path createScratchDir()>",
    "<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void flushToFile()>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: java.lang.String getSha(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: void writeToFile(java.lang.String,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.antlr.runtime.tree.Tree,boolean)>",
    "<org.apache.hadoop.hive.ql.session.SessionState: void createPath(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,java.lang.String,boolean,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.orc.tools.JsonFileDump: void printJsonMetaData(java.util.List,org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>",
    "<org.apache.hadoop.hive.shims.Hadoop23Shims$HCatHadoopShims23: java.net.InetSocketAddress getResourceManagerAddress(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.shims.Utils: org.apache.hadoop.security.UserGroupInformation getUGI()>",
    "<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.IMetaStoreClient getMSC(boolean,boolean)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: long renewDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.Context: java.io.DataInput getStream()>",
    "<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: java.lang.String generateCmdLine(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.Context)>",
    "<org.apache.hadoop.hive.ql.optimizer.IndexUtils: boolean isIndexTableFresh(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>",
    "<org.apache.parquet.hadoop.CodecFactory$BytesDecompressor: void <init>(org.apache.parquet.hadoop.CodecFactory,org.apache.hadoop.io.compress.CompressionCodec)>",
    "<org.apache.hadoop.hive.ql.exec.mr.HashTableLoader: org.apache.hadoop.fs.Path getBaseDir(org.apache.hadoop.hive.ql.plan.MapredLocalWork)>",
    "<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void writeOutput(java.util.List,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void clean(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>",
    "<org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>",
    "<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.shims.ShimLoader: java.lang.String getMajorVersion()>",
    "<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: void writePartitionKeys(org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.DDLTask: int archive(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc,org.apache.hadoop.hive.ql.DriverContext)>",
    "<org.apache.hadoop.hive.ql.exec.StatsNoJobTask$StatsCollection: void run()>",
    "<org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator: void setConf(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void processFiles(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>",
    "<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: java.lang.String getSha(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.thrift.DelegationTokenSecretManager: java.lang.String getDelegationToken(java.lang.String)>",
    "<org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver: java.lang.String getURL(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void openInternal(org.apache.hadoop.hive.conf.HiveConf,java.util.Collection,boolean,org.apache.hadoop.hive.ql.session.SessionState$LogHelper,org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer: java.util.Map aggregateStats(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void handleSampling(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.orc.impl.ReaderImpl: void <init>(org.apache.hadoop.fs.Path,org.apache.orc.OrcFile$ReaderOptions)>",
    "<org.apache.orc.tools.FileDump: org.apache.orc.Reader getReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.List)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void fixLocationInPartSpec(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.plan.AddPartitionDesc$OnePartitionDesc,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean isPathEncrypted(org.apache.hadoop.fs.Path)>",
    "<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext)>",
    "<org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Client$1: org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport run()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.yarn.api.records.LocalResource localizeResource(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void clear()>",
    "<org.apache.avro.mapred.AvroMultipleOutputs: org.apache.hadoop.mapred.RecordWriter getRecordWriter(java.lang.String,java.lang.String,org.apache.hadoop.mapred.Reporter,org.apache.avro.Schema)>",
    "<org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator: void initializeMapredLocalWork(org.apache.hadoop.hive.ql.plan.MapJoinDesc,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.MapredLocalWork,org.slf4j.Logger)>",
    "<org.apache.hadoop.hive.ql.exec.spark.SparkUtilities: java.net.URI uploadToHDFS(java.net.URI,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.util.List first()>",
    "<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.tez.dag.api.Vertex createVertex(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.yarn.api.records.LocalResource,java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.TezWork$VertexType)>",
    "<org.apache.parquet.hadoop.PrintFooter: void main(java.lang.String[])>",
    "<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidTxnList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[])>",
    "<org.apache.hadoop.hive.ql.exec.spark.MapInput$CopyFunction: scala.Tuple2 call(scala.Tuple2)>",
    "<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy: void runGetSplitsSync(java.util.List,java.util.List,org.apache.hadoop.security.UserGroupInformation)>",
    "<org.apache.parquet.hadoop.ParquetFileReader: org.apache.parquet.hadoop.metadata.ParquetMetadata readSummaryMetadata(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean)>",
    "<org.apache.hadoop.hive.ql.exec.mr.ExecDriver: void main(java.lang.String[])>"
  ]
}