{
  "1.1.2": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.4": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.2.0": [
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.6": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.3": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.0": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.2.1": [
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.1": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.2": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.2.2": [
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.2.4": [
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.0.5": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.2.3": [
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.1.3": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.1.0": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ],
  "1.1.1": [
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: org.apache.storm.hdfs.common.AbstractHDFSWriter makeNewWriter(org.apache.hadoop.fs.Path,org.apache.storm.tuple.Tuple)>",
    "<org.apache.storm.hdfs.bolt.SequenceFileBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void addTokensToUGI(javax.security.auth.Subject)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: org.apache.hadoop.fs.Path createOutputFile()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: byte[] getHadoopCredentials(java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: javax.security.auth.Subject getHadoopUser()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS: void login(org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: long getModTime()>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreImpl: void <init>(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.common.security.HdfsSecurityUtil: void login(java.util.Map,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.spout.HdfsSpout: void open(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.spout.SpoutOutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void commit()>",
    "<org.apache.storm.hdfs.common.security.AutoHDFS$1: java.lang.Object run()>",
    "<org.apache.storm.hdfs.bolt.HdfsBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStore: void prepareInternal(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.trident.HdfsState$HdfsFileOptions: void doPrepare(java.util.Map,int,int)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)>",
    "<org.apache.storm.hdfs.bolt.AvroGenericRecordBolt: void doPrepare(java.util.Map,org.apache.storm.task.TopologyContext,org.apache.storm.task.OutputCollector)>",
    "<org.apache.storm.hdfs.trident.HdfsState$SequenceFileOptions: void doRecover(org.apache.hadoop.fs.Path,long)>",
    "<org.apache.storm.hdfs.spout.SequenceFileReader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map,java.lang.String)>",
    "<org.apache.storm.hdfs.blobstore.HdfsBlobStoreFile: void <init>(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration)>"
  ]
}