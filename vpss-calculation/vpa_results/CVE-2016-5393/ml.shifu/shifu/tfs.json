{
  "0.12.0": [
    "<ml.shifu.shifu.core.correlation.FastCorrelationMultithreadedMapper$SubMapRecordReader: boolean nextKeyValue()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.wdl.WDLOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.udf.ColumnProjector: void finish()>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem renewFS()>",
    "<ml.shifu.shifu.udf.EvalScoreUDF: void finish()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeBinaryModelWeightsToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem getFS()>",
    "<ml.shifu.shifu.guagua.ShifuInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.mapreduce.JobContext)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput$1: void run()>",
    "<ml.shifu.shifu.core.mr.input.CombineInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<ml.shifu.shifu.core.mr.input.CombineRecordReader: void initializeOne(org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<ml.shifu.shifu.core.dtrain.wdl.WDLOutput: void writeModelToFileSystem(ml.shifu.shifu.core.dtrain.wdl.WDLParams,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.stats.MapReducerStatsWorker: void prepareJobConf(ml.shifu.shifu.container.obj.RawSourceData$SourceType,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<ml.shifu.shifu.core.processor.VarSelectModelProcessor: void prepareSEJobConf(ml.shifu.shifu.container.obj.RawSourceData$SourceType,org.apache.hadoop.conf.Configuration)>",
    "<ml.shifu.shifu.core.processor.PostTrainModelProcessor: void runMRFeatureImportanceJob(ml.shifu.shifu.container.obj.RawSourceData$SourceType,java.lang.String)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTMaster: void writeStatesToHdfs(org.apache.hadoop.fs.Path,ml.shifu.shifu.core.dtrain.dt.DTMasterParams,java.util.List,boolean,java.util.Queue,java.util.Queue)>",
    "<ml.shifu.shifu.core.dtrain.wdl.WDLOutput$1: void run()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput$2: void run()>",
    "<ml.shifu.shifu.core.dtrain.wdl.WDLOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.TrainModelProcessor: int runDistributedTrain()>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem getLocalFS()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeEncogModelToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.processor.PostTrainModelProcessor: void runMRBinAvgScoreJob(ml.shifu.shifu.container.obj.RawSourceData$SourceType,java.lang.String)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void writeModelWeightsToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.shuffle.MapReduceShuffle: void run(java.lang.String)>",
    "<ml.shifu.shifu.core.processor.ExportModelProcessor: int run()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void writeModelToFileSystem(java.util.List,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.StatsModelProcessor: void runCorrMapReduceJob()>",
    "<ml.shifu.shifu.core.correlation.CorrelationMultithreadedMapper$SubMapRecordReader: boolean nextKeyValue()>",
    "<ml.shifu.shifu.core.processor.InitModelProcessor: java.util.Map getCountInfoByMRJob()>"
  ],
  "0.11.1": [
    "<ml.shifu.shifu.core.correlation.FastCorrelationMultithreadedMapper$SubMapRecordReader: boolean nextKeyValue()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.udf.ColumnProjector: void finish()>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem renewFS()>",
    "<ml.shifu.shifu.udf.EvalScoreUDF: void finish()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeBinaryModelWeightsToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem getFS()>",
    "<ml.shifu.shifu.guagua.ShifuInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.mapreduce.JobContext)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput$1: void run()>",
    "<ml.shifu.shifu.core.mr.input.CombineInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<ml.shifu.shifu.core.mr.input.CombineRecordReader: void initializeOne(org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<ml.shifu.shifu.core.processor.stats.MapReducerStatsWorker: void prepareJobConf(ml.shifu.shifu.container.obj.RawSourceData$SourceType,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<ml.shifu.shifu.core.processor.VarSelectModelProcessor: void prepareSEJobConf(ml.shifu.shifu.container.obj.RawSourceData$SourceType,org.apache.hadoop.conf.Configuration)>",
    "<ml.shifu.shifu.core.processor.PostTrainModelProcessor: void runMRFeatureImportanceJob(ml.shifu.shifu.container.obj.RawSourceData$SourceType,java.lang.String)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTMaster: void writeStatesToHdfs(org.apache.hadoop.fs.Path,ml.shifu.shifu.core.dtrain.dt.DTMasterParams,java.util.List,boolean,java.util.Queue,java.util.Queue)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput$2: void run()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.TrainModelProcessor: int runDistributedTrain()>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem getLocalFS()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeEncogModelToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.processor.PostTrainModelProcessor: void runMRBinAvgScoreJob(ml.shifu.shifu.container.obj.RawSourceData$SourceType,java.lang.String)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void writeModelWeightsToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.shuffle.MapReduceShuffle: void run(java.lang.String)>",
    "<ml.shifu.shifu.core.processor.ExportModelProcessor: int run()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void writeModelToFileSystem(java.util.List,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.StatsModelProcessor: void runCorrMapReduceJob()>",
    "<ml.shifu.shifu.core.correlation.CorrelationMultithreadedMapper$SubMapRecordReader: boolean nextKeyValue()>",
    "<ml.shifu.shifu.core.processor.InitModelProcessor: java.util.Map getCountInfoByMRJob()>"
  ],
  "0.11.2": [
    "<ml.shifu.shifu.core.correlation.FastCorrelationMultithreadedMapper$SubMapRecordReader: boolean nextKeyValue()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.udf.ColumnProjector: void finish()>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem renewFS()>",
    "<ml.shifu.shifu.udf.EvalScoreUDF: void finish()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeBinaryModelWeightsToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem getFS()>",
    "<ml.shifu.shifu.guagua.ShifuInputFormat: org.apache.hadoop.fs.Path[] getInputPaths(org.apache.hadoop.mapreduce.JobContext)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput$1: void run()>",
    "<ml.shifu.shifu.core.mr.input.CombineInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>",
    "<ml.shifu.shifu.core.mr.input.CombineRecordReader: void initializeOne(org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>",
    "<ml.shifu.shifu.core.processor.stats.MapReducerStatsWorker: void prepareJobConf(ml.shifu.shifu.container.obj.RawSourceData$SourceType,org.apache.hadoop.conf.Configuration,java.lang.String)>",
    "<ml.shifu.shifu.core.processor.VarSelectModelProcessor: void prepareSEJobConf(ml.shifu.shifu.container.obj.RawSourceData$SourceType,org.apache.hadoop.conf.Configuration)>",
    "<ml.shifu.shifu.core.processor.PostTrainModelProcessor: void runMRFeatureImportanceJob(ml.shifu.shifu.container.obj.RawSourceData$SourceType,java.lang.String)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTMaster: void writeStatesToHdfs(org.apache.hadoop.fs.Path,ml.shifu.shifu.core.dtrain.dt.DTMasterParams,java.util.List,boolean,java.util.Queue,java.util.Queue)>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput$2: void run()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeValErrorToFileSystem(double,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.TrainModelProcessor: int runDistributedTrain()>",
    "<ml.shifu.shifu.util.HDFSUtils: org.apache.hadoop.fs.FileSystem getLocalFS()>",
    "<ml.shifu.shifu.core.dtrain.nn.NNOutput: void writeEncogModelToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void init(ml.shifu.guagua.master.MasterContext)>",
    "<ml.shifu.shifu.core.processor.PostTrainModelProcessor: void runMRBinAvgScoreJob(ml.shifu.shifu.container.obj.RawSourceData$SourceType,java.lang.String)>",
    "<ml.shifu.shifu.core.dtrain.lr.LogisticRegressionOutput: void writeModelWeightsToFileSystem(double[],org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.shuffle.MapReduceShuffle: void run(java.lang.String)>",
    "<ml.shifu.shifu.core.processor.ExportModelProcessor: int run()>",
    "<ml.shifu.shifu.core.dtrain.dt.DTOutput: void writeModelToFileSystem(java.util.List,org.apache.hadoop.fs.Path)>",
    "<ml.shifu.shifu.core.processor.StatsModelProcessor: void runCorrMapReduceJob()>",
    "<ml.shifu.shifu.core.correlation.CorrelationMultithreadedMapper$SubMapRecordReader: boolean nextKeyValue()>",
    "<ml.shifu.shifu.core.processor.InitModelProcessor: java.util.Map getCountInfoByMRJob()>"
  ]
}