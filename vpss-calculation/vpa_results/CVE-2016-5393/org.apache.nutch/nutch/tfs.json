{
  "1.13": [
    "<org.apache.nutch.crawl.CrawlDbReader: void main(java.lang.String[])>",
    "<org.apache.nutch.indexer.IndexerMapReduce: void initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf,boolean)>",
    "<org.apache.nutch.protocol.Content: void main(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.NodeDumper: void main(java.lang.String[])>",
    "<org.apache.nutch.tools.Benchmark: org.apache.nutch.tools.Benchmark$BenchmarkResults benchmark(int,int,int,int,long,boolean,java.lang.String)>",
    "<org.apache.nutch.indexer.CleaningJob: void main(java.lang.String[])>",
    "<org.apache.nutch.indexer.IndexingJob: void index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean,boolean,boolean)>",
    "<org.apache.nutch.crawl.Generator: void main(java.lang.String[])>",
    "<org.apache.nutch.util.ProtocolStatusStatistics: void main(java.lang.String[])>",
    "<org.apache.nutch.crawl.LinkDbMerger: void main(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.NodeDumper$Sorter: void reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.scoring.webgraph.LinkRank$Analyzer: void reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.scoring.webgraph.LinkDumper: void dumpLinks(org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.segment.SegmentMerger: void merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long)>",
    "<org.apache.nutch.crawl.DeduplicationJob: void main(java.lang.String[])>",
    "<org.apache.nutch.tools.warc.WARCExporter: void main(java.lang.String[])>",
    "<org.apache.nutch.segment.SegmentReader: void <init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.nutch.hostdb.UpdateHostDb: void main(java.lang.String[])>",
    "<org.apache.nutch.segment.SegmentReader: void main(java.lang.String[])>",
    "<org.apache.nutch.protocol.RobotRulesParser: void main(java.lang.String[])>",
    "<org.apache.nutch.service.impl.LinkReader: int count(java.lang.String)>",
    "<org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1: org.apache.hadoop.io.SequenceFile$Writer ensureSequenceFile(java.lang.String,java.lang.String)>",
    "<org.apache.nutch.service.impl.NodeReader: int count(java.lang.String)>",
    "<org.apache.nutch.scoring.webgraph.LinkRank: void main(java.lang.String[])>",
    "<org.apache.nutch.hostdb.ReadHostDb: void main(java.lang.String[])>",
    "<org.apache.nutch.indexer.IndexingJob: java.util.Map run(java.util.Map,java.lang.String)>",
    "<org.apache.nutch.crawl.CrawlDb: void main(java.lang.String[])>",
    "<org.apache.nutch.service.impl.SequenceReader: java.util.List read(java.lang.String)>",
    "<org.apache.nutch.crawl.CrawlDbMerger: void merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)>",
    "<org.apache.nutch.crawl.LinkDb: org.apache.hadoop.mapred.JobConf createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.nutch.service.resources.SeedResource: java.lang.String writeToSeedFile(java.util.Collection)>",
    "<org.apache.nutch.crawl.LinkDb: java.util.Map run(java.util.Map,java.lang.String)>",
    "<org.apache.nutch.crawl.LinkDbReader: void main(java.lang.String[])>",
    "<org.apache.nutch.indexer.IndexingJob: void main(java.lang.String[])>",
    "<org.apache.nutch.hostdb.UpdateHostDb: void updateHostDb(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,boolean,boolean,boolean)>",
    "<org.apache.nutch.scoring.webgraph.LinkDumper$Inverter: void reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.crawl.CrawlDb: void install(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.tools.Benchmark: void main(java.lang.String[])>",
    "<org.apache.nutch.crawl.CrawlDbMerger: int run(java.lang.String[])>",
    "<org.apache.nutch.segment.SegmentReader: void configure(org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.nutch.scoring.webgraph.LinkRank: void analyze(org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.segment.SegmentMerger: void main(java.lang.String[])>",
    "<org.apache.nutch.service.impl.LinkReader: java.util.List slice(java.lang.String,int,int)>",
    "<org.apache.nutch.scoring.webgraph.LinkRank$Initializer: void map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.tools.arc.ArcSegmentCreator: void main(java.lang.String[])>",
    "<org.apache.nutch.indexer.IndexingFiltersChecker: void main(java.lang.String[])>",
    "<org.apache.nutch.tools.CommonCrawlDataDumper: void dump(java.io.File,java.io.File,java.io.File,boolean,java.lang.String[],boolean,java.lang.String,boolean)>",
    "<org.apache.nutch.scoring.webgraph.ScoreUpdater: void update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.service.impl.SequenceReader: java.util.List head(java.lang.String,int)>",
    "<org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb: void reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.parse.ParseText: void main(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.LinkRank$Analyzer: void map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.tools.FreeGenerator: void main(java.lang.String[])>",
    "<org.apache.nutch.indexer.IndexingJob: int run(java.lang.String[])>",
    "<org.apache.nutch.parse.ParseOutputFormat: org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)>",
    "<org.apache.nutch.parse.ParserChecker: void main(java.lang.String[])>",
    "<org.apache.nutch.fetcher.FetcherOutputFormat: org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)>",
    "<org.apache.nutch.crawl.DeduplicationJob: int run(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.WebGraph: void createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)>",
    "<org.apache.nutch.scoring.webgraph.NodeReader: void dumpUrl(org.apache.hadoop.fs.Path,java.lang.String)>",
    "<org.apache.nutch.crawl.LinkDb: int run(java.lang.String[])>",
    "<org.apache.nutch.crawl.Generator: org.apache.hadoop.fs.Path[] generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int,java.lang.String)>",
    "<org.apache.nutch.service.impl.SequenceReader: java.util.List slice(java.lang.String,int,int)>",
    "<org.apache.nutch.scoring.webgraph.WebGraph: int run(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.LinkRank$Inverter: void reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.crawl.CrawlDbReader: void processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1: org.apache.hadoop.io.MapFile$Writer ensureMapFile(java.lang.String,java.lang.String,java.lang.Class)>",
    "<org.apache.nutch.service.impl.SequenceReader: int count(java.lang.String)>",
    "<org.apache.nutch.crawl.CrawlDb: void update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean)>",
    "<org.apache.nutch.parse.ParseSegment: void main(java.lang.String[])>",
    "<org.apache.nutch.service.impl.NodeReader: java.util.List read(java.lang.String)>",
    "<org.apache.nutch.service.impl.NodeReader: java.util.List slice(java.lang.String,int,int)>",
    "<org.apache.nutch.crawl.Injector: void inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>",
    "<org.apache.nutch.parse.ParseSegment: void parse(org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.crawl.CrawlDbReader: java.util.TreeMap processStatJobHelper(java.lang.String,org.apache.hadoop.conf.Configuration,boolean)>",
    "<org.apache.nutch.tools.FileDumper: void dump(java.io.File,java.io.File,java.lang.String[],boolean,boolean,boolean)>",
    "<org.apache.nutch.scoring.webgraph.LinkDumper$Reader: void main(java.lang.String[])>",
    "<org.apache.nutch.parse.ParseData: void main(java.lang.String[])>",
    "<org.apache.nutch.fetcher.Fetcher: void main(java.lang.String[])>",
    "<org.apache.nutch.crawl.LinkDbReader: void init(org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.crawl.CrawlDbMerger: void main(java.lang.String[])>",
    "<org.apache.nutch.crawl.CrawlDbReader: void openReaders(java.lang.String,org.apache.hadoop.mapred.JobConf)>",
    "<org.apache.nutch.service.impl.LinkReader: java.util.List head(java.lang.String,int)>",
    "<org.apache.nutch.crawl.LinkDb: void main(java.lang.String[])>",
    "<org.apache.nutch.tools.warc.WARCExporter: int run(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.LinkDumper: void main(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.ScoreUpdater: void main(java.lang.String[])>",
    "<org.apache.nutch.crawl.Injector: void main(java.lang.String[])>",
    "<org.apache.nutch.scoring.webgraph.WebGraph: void main(java.lang.String[])>",
    "<org.apache.nutch.crawl.LinkDb: void invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)>",
    "<org.apache.nutch.crawl.CrawlDb: java.util.Map run(java.util.Map,java.lang.String)>",
    "<org.apache.nutch.crawl.CrawlDb: int run(java.lang.String[])>",
    "<org.apache.nutch.segment.SegmentMerger: int run(java.lang.String[])>",
    "<org.apache.nutch.fetcher.FetcherOutputFormat$1: void <init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.MapFile$Writer)>",
    "<org.apache.nutch.crawl.LinkDb: void invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean)>",
    "<org.apache.nutch.scoring.webgraph.LinkDumper$Merger: void reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.segment.SegmentMerger$ObjectInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>",
    "<org.apache.nutch.service.impl.NodeReader: java.util.List head(java.lang.String,int)>",
    "<org.apache.nutch.util.CrawlCompletionStats: void main(java.lang.String[])>",
    "<org.apache.nutch.tools.DmozParser: void main(java.lang.String[])>",
    "<org.apache.nutch.util.domain.DomainStatistics: void main(java.lang.String[])>",
    "<org.apache.nutch.segment.SegmentReader: int run(java.lang.String[])>",
    "<org.apache.nutch.service.impl.LinkReader: java.util.List read(java.lang.String)>",
    "<org.apache.nutch.crawl.CrawlDb: org.apache.hadoop.mapred.JobConf createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>",
    "<org.apache.nutch.crawl.LinkDbMerger: void merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)>",
    "<org.apache.nutch.tools.CommonCrawlDataDumper: void main(java.lang.String[])>"
  ]
}