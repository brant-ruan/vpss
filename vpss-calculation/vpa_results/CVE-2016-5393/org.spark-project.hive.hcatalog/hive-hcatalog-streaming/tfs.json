{
  "1.2.1.spark2": [
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl: void createPartitionIfNotExists(org.apache.hive.hcatalog.streaming.HiveEndPoint,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl: org.apache.hadoop.hive.metastore.IMetaStoreClient getMetaStoreClient(org.apache.hive.hcatalog.streaming.HiveEndPoint,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint: org.apache.hadoop.hive.conf.HiveConf createHiveConf(java.lang.Class,java.lang.String)>",
    "<org.apache.hive.hcatalog.streaming.AbstractRecordWriter: void <init>(org.apache.hive.hcatalog.streaming.HiveEndPoint,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl: boolean runDDL(org.apache.hadoop.hive.ql.Driver,java.lang.String)>"
  ],
  "1.2.1.spark": [
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl: void createPartitionIfNotExists(org.apache.hive.hcatalog.streaming.HiveEndPoint,org.apache.hadoop.hive.metastore.IMetaStoreClient,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl: org.apache.hadoop.hive.metastore.IMetaStoreClient getMetaStoreClient(org.apache.hive.hcatalog.streaming.HiveEndPoint,org.apache.hadoop.hive.conf.HiveConf,boolean)>",
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint: org.apache.hadoop.hive.conf.HiveConf createHiveConf(java.lang.Class,java.lang.String)>",
    "<org.apache.hive.hcatalog.streaming.AbstractRecordWriter: void <init>(org.apache.hive.hcatalog.streaming.HiveEndPoint,org.apache.hadoop.hive.conf.HiveConf)>",
    "<org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl: boolean runDDL(org.apache.hadoop.hive.ql.Driver,java.lang.String)>"
  ]
}